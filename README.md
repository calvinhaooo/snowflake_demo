# ELT Pipeline with Snowflake, dbt, and Airflow

This repository contains an end-to-end ELT pipeline built using Snowflake, dbt, Airflow, and Cosmos.  
The project demonstrates how to model, transform, and orchestrate data in a modern analytics engineering workflow.

## Project Structure

```mermaid
flowchart LR

    %% --- Raw Layer ---
    A[ðŸƒ CSV / TPCH Raw Data] --> B

    %% --- Airflow ingestion ---
    B[[ðŸŒ€ Airflow<br/>Load Raw Data into Snowflake]] --> C

    %% --- Staging + Quality Checks ---
    C --> D((ðŸ” Quality Checks))
    D --> E[[ðŸ—ï¸ dbt Staging Models]]

    %% --- Intermediate + Quality Checks ---
    E --> F((ðŸ” Quality Checks))
    F --> G[[ðŸ”§ dbt Intermediate Models]]

    %% --- Fact / Mart + Quality Checks ---
    G --> H((ðŸ” Quality Checks))
    H --> I[[ðŸ“Š dbt Fact & Mart Models]]

    %% --- BI Layer ---
    I --> J[ðŸ“ˆ BI Dashboard / Reporting]

    %% --- Optional blue outline like your diagram ---
    classDef pipeline stroke:#3fa9f5,stroke-width:2px,stroke-dasharray:6 4,fill:none;
    class B,D,E,F,G,H,I pipeline;
```



Below are the key directories and files of this project (non-essential auto-generated folders omitted):

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ airflow_settings.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ packages.txt
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â””â”€â”€ dbt/
â”‚       â””â”€â”€ snowflake_demo/
â”‚           â”œâ”€â”€ dbt_project.yml
â”‚           â”œâ”€â”€ packages.yml
â”‚           â”œâ”€â”€ models/
â”‚           â”‚   â”œâ”€â”€ staging/
â”‚           â”‚   â”‚   â”œâ”€â”€ stg_tpch_orders.sql
â”‚           â”‚   â”‚   â”œâ”€â”€ stg_tcph_line_items.sql
â”‚           â”‚   â”‚   â””â”€â”€ tpch_sources.yml
â”‚           â”‚   â””â”€â”€ marts/
â”‚           â”‚       â”œâ”€â”€ int_order_items.sql
â”‚           â”‚       â”œâ”€â”€ int_order_items_summary.sql
â”‚           â”‚       â”œâ”€â”€ fct_orders.sql
â”‚           â”‚       â””â”€â”€ generic_test.yml
â”‚           â”‚
â”‚           â”œâ”€â”€ macros/
â”‚           â”‚   â””â”€â”€ pricing.sql
â”‚           â”‚
â”‚           â”œâ”€â”€ tests/
â”‚           â”‚   â”œâ”€â”€ fct_orders_discount.sql
â”‚           â”‚   â””â”€â”€ fct_orders_orderdate.sql
â”‚           â”‚
â”‚           â””â”€â”€ seeds/ (empty)
â”‚
â””â”€â”€ README.md
```

## Components

### dbt Models
- **staging/**  
  Cleans, renames, and standardizes raw TPCH data.

- **marts/**  
  Contains intermediate models and the main fact table `fct_orders`.

- **macros/**  
  Reusable SQL logic, including a pricing macro.

- **tests/**  
  Custom singular tests validating `fct_orders`.

### Airflow DAG
`dags/dbt_dag.py` orchestrates dbt runs using Cosmos.

### Configuration Files
- `dbt_project.yml` â€” dbt settings  
- `packages.yml` â€” dbt dependencies  
- `Dockerfile` â€” Airflow image customization  
- `requirements.txt` â€” Python dependencies  


## Running airflow

```
# This line will start the project
astro dev start

# This line will stop the project
astro dev stop
```

## Running dbt

```
dbt run
dbt test
dbt run -s staging_file
dbt run -s marts_file
```

## Example Snowflake Profile

```
snowflake_demo:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: <your_account>.europe-west4.gcp
      user: <your_user>
      password: <your_password>
      role: <your_role>
      warehouse: DBT_WH
      database: DBT_DB
      schema: DBT_SCHEMA
```

## Purpose

This project serves as a minimal but complete example of how to build a production-style ELT workflow using:

- Snowflake for storage  
- dbt for transformation and data modeling  
- Airflow (Cosmos) for orchestration  

It is intended as a reference implementation for learning or extending into a full data platform.

